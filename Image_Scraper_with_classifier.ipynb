{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A943k0ypc5T"
      },
      "outputs": [],
      "source": [
        "#importing the required Libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen as ureq\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import csv\n",
        "import shutil\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "#creating the directory for our data\n",
        "save_dir = \"images/\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"}\n",
        "\n",
        "#Taking the iput from user\n",
        "query_input = []\n",
        "for i in input(\"enter the name:\").split():\n",
        "  query_input.append(i)\n",
        "\n",
        "#Removing the unwanted class folder\n",
        "\n",
        "#Scrapping the images from the web\n",
        "for i in range(0,len(query_input)):\n",
        "\n",
        "  query = query_input[i]\n",
        "  response = requests.get(f\"https://www.google.com/search?q={query}&client=firefox-b-d&sca_esv=573422471&tbm=isch&source=lnms&sa=X&ved=2ahUKEwjjz_qimvWBAxWX2TgGHb31CdkQ_AUoAnoECAMQBA&biw=1451&bih=684&dpr=1.76\")\n",
        "\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "  image_tag = soup.find_all(\"img\")\n",
        "  del image_tag[0]\n",
        "  for i in image_tag:\n",
        "        image_url = i['src']\n",
        "        image_data = requests.get(image_url).content\n",
        "        my_dir = {\"index\": image_url, \"image\": image_data}\n",
        "\n",
        "        #saving the image data in local dictary\n",
        "        class_name = save_dir+query+\"/\"\n",
        "        if not os.path.exists(class_name):\n",
        "          os.makedirs(class_name)\n",
        "        with open(os.path.join(class_name, f\"{query}{image_tag.index(i)}.jpg\"), \"wb\") as f:\n",
        "          f.write(image_data)\n",
        "  for i in tqdm(range(100)):\n",
        "    sleep(0.05)\n",
        "    pass\n",
        "  print(f\"Download Complete For {query}\")\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 200\n",
        "img_width = 200\n",
        "\n",
        "#defining the path of our images\n",
        "data_set = pathlib.Path(\"/content/images\")\n",
        "\n",
        "#Spliting the data for training with 80%\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_set,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "#Spliting the data for validation with 20%\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_set,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n",
        "\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "num_classes = len(class_names)\n",
        "\n",
        "model = Sequential([\n",
        "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "epochs=12\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "with plt.style.context(('dark_background')):\n",
        "  plt.figure(figsize=(12, 12))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs_range, acc,'r-o', label='Training Accuracy')\n",
        "  plt.plot(epochs_range,val_acc,'y-o', label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs_range,loss,'r-o', label='Training Loss')\n",
        "  plt.plot(epochs_range,val_loss,'y-o', label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title('Training and Validation Loss')\n",
        "\n",
        "  plt.savefig(\"Result.jpg\", dpi=1080)\n",
        "plt.show()\n",
        "test_url = input(\"Enter the url of image\")\n",
        "test_path = tf.keras.utils.get_file(origin=test_url)\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    test_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        ")\n",
        "\n",
        "#Saving the data future reference\n",
        "user_input = ' '.join(query_input)\n",
        "Accuracy = 100 * np.max(score)\n",
        "data= [user_input,test_url, Accuracy]\n",
        "data_for_csv = [[\"User Input\", \"Test Url\", \"Accuracy Achieved\"], data]\n",
        "\n",
        "if not os.path.exists(\"Results\"):\n",
        "  os.makedirs(\"Results\")\n",
        "with open(\"/content/Results/Data_records.csv\",\"w\") as f:\n",
        "  writer = csv.writer(f)\n",
        "  for i in data_for_csv:\n",
        "    writer.writerow(i)\n",
        "\n",
        "#Deleting the dataset\n",
        "try:\n",
        "    shutil.rmtree(\"/content/images\")\n",
        "    print(\"Directory removed successfully\")\n",
        "\n",
        "except OSError as o:\n",
        "    print(f\"Error, {o.strerror}: {path}\")\n"
      ]
    }
  ]
}